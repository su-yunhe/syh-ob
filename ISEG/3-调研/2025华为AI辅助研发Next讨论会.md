# 2025华为AI辅助研发Next讨论会

## 一、面向AI辅助编程的可解释性技术分析

> 林云 上海交通大学

探讨集中在两个方面：一是表征分析或编码分析，旨在理解知识在模型内部的编码方式；二是数据归因，关注如何从预测结果反馈到训练数据集以进行矫正，强调数据清理和标注的重要性。未来软件工程的交付和编码过程将越来越多地涉及数据标注，有效整理和归因变得尤为重要。
![[Pasted image 20250117114523.png]]

### 1. 表征（编码）分析

深度学习模型的核心在于表征学习，即如何将输入数据（如图像或文本）转化为高维向量，从而在高维空间中进行有效分类和多模态学习。通过分析，我们发现每个训练样本在高维空间有其独特的表征，这使得模型能够学习到分类边界，如区分猫和狗的图片。同时，探讨了高维空间表征的理解挑战和可视化训练框架，以揭示模型学习过程中表征的形成过程。
 ![[Pasted image 20250117114543.png]]


讨论集中在将模型训练过程转换为可交互动画上，目的在于增强模型训练过程的可观测性，使人们能够理解表征空间的形成过程，并基于此进行推理、分析和查询。通过可视化，可以观察到不同样本在训练过程中的表征变化，比如正确分类的样本和误分类的样本的运动模式，以及它们之间的语义相近性。这种可视化的分析有助于识别训练中的异常情况，比如干净样本和噪声样本的预测差异，为预训练模型的分析提供了新的视角。此研究最终发表在KDD会议上，强调了可视化在理解复杂模型训练过程中的重要性。
 ![[Pasted image 20250117114625.png]]
 ![[Pasted image 20250117114647.png]]
 ![[Pasted image 20250117114702.png]]


在深度学习中，要理解和可视化高维空间的复杂操作，特别是关注于dropout技术如何平滑分类边界。此外，代码检索任务，将不同模态的样本投射到同一空间上的重要性，以及在这一过程中可视化技术的潜在应用。
 ![[Pasted image 20250117114723.png]]![[Pasted image 20250117114740.png]]
 
 
 
在对CoderBERT的分析中发现，通过Transformer解码得到的表征，虽然能够基于COS相似度进行搜索和匹配，但这种表征的效用和质量难以直接评判。实验显示，这种表征在训练数据集上表现良好，但在未知数据集上的性能显著下降，揭示了模型学习到的更多是相关性而非因果性。此现象在代码和注释的匹配中尤为明显，代码的表征学习往往基于表面特征而非深层次的语义理解，导致在某些情况下无法与人类的直观理解相匹配。为解决此问题，开发了一种工具，通过分析token对总表示的贡献度和背后的概念，尝试在代码和其描述之间建立更准确的对齐关系，从而改善代码表征学习的质量。
 
 ![[Pasted image 20250117114755.png]]
 ![[Pasted image 20250117114818.png]]
 ![[Pasted image 20250117114840.png]]
 ![[Pasted image 20250117114852.png]]
 ![[Pasted image 20250117114936.png]]
 ![[Pasted image 20250117114949.png]]
![[Pasted image 20250117114959.png]]



介绍了一种利用GPT进行代码标注的创新方法，通过积累手动标注的数据并利用大语言模型进行二次标注，循环迭代以提高数据集的标注效率和质量。此方法旨在加强自然语言描述与代码之间的对应关系，引入注意力机制和损失函数优化标注效果，从而提升代码的可解释性和检索效率。此外，通过减少样本量并增强模型泛化能力，该方法能够在未见数据集上表现更佳。

![[Pasted image 20250117115012.png]]
![[Pasted image 20250117115042.png]]

![[Pasted image 20250117115059.png]]
![[Pasted image 20250117115202.png]]
![[Pasted image 20250117115212.png]]
### 2. 训练数据归因分析

介绍了一种与字节跳动合作开发的代码编辑技术，该技术通过分析commit message和之前的编辑内容，在项目中定位编辑位置并生成编辑内容的候选项。这项技术首先通过位于模型预测编辑类型，然后利用transformer生成具体编辑内容，最终目的是实现AI for SE技术的工程化和落地化。
![[Pasted image 20250117115404.png]]
![[Pasted image 20250117115429.png]]
![[Pasted image 20250117115443.png]]
![[Pasted image 20250117115459.png]]
![[Pasted image 20250117115552.png]]
![[Pasted image 20250117115602.png]]


影响函数用于追踪推荐结果在训练数据上的贡献，通过调整训练样本的权重来观察模型变化，帮助发现标注错误和优化模型预测。
![[Pasted image 20250117115613.png]]
![[Pasted image 20250117115626.png]]

### 3. 总结


![[Pasted image 20250117115700.png]]
![[Pasted image 20250117115716.png]]

## 二、数据工程对于代码大模型的影响
> 高翠芸 哈工（深）

![[Pasted image 20250117120349.png]]
![[Pasted image 20250117120457.png]]

![[Pasted image 20250117120517.png]]
![[Pasted image 20250117120545.png]]
![[Pasted image 20250117120558.png]]

### 1. 数据的质量如何评估？

![[Pasted image 20250117120618.png]]



当前大模型技术报告中对微调数据的描述通常模糊，提及的数据质量和构成方式不够明确。有研究指出，指令数据的量并非越多越好，重要的是数据的质量。研究通过三个维度（指令的复杂度、回复质量、指令多样性）评估数据质量，并采用自动化方式评判，结果表明少量高质量数据能达到更佳的训练效果。此外，研究还探索了代码部分对模型效果的影响，为如何选择和构成高质量训练数据提供了启发。
![[Pasted image 20250117120632.png]]
![[Pasted image 20250117120647.png]]
![[Pasted image 20250117120659.png]]



研究关注于通过增加数据集的多样性和领域覆盖，来提升代码大模型的性能。通过将不同领域的数据集（包含常识、推理及数学等）融合至指令数据集中，观察到性能的显著提高。特别地，通过加入额外的数据集（，进一步提升了模型性能。此外，研究从数据大小、多样性、困惑度和来源等多维度评估指令微调数据的质量，发现控制数据困惑度和涵盖广泛的数据源对提高模型性能至关重要。基于这些发现，开发了一个小规模混合领域指令数据集，该数据集在多个任务上表现出优于原始数据集的性能。此研究仍在持续优化中。
![[Pasted image 20250117120714.png]]
![[Pasted image 20250117120724.png]]
![[Pasted image 20250117120759.png]]
![[Pasted image 20250117120829.png]]


### 2. 如何基于数据进行SFT/调优？
![[Pasted image 20250117120847.png]]
![[Pasted image 20250117120856.png]]
![[Pasted image 20250117120938.png]]
![[Pasted image 20250117121005.png]]
![[Pasted image 20250117121029.png]]
![[Pasted image 20250117121103.png]]
![[Pasted image 20250117121114.png]]


### 3. 利用哪些数据来上下文学习？
![[Pasted image 20250117121130.png]]
![[Pasted image 20250117121141.png]]
![[Pasted image 20250117121154.png]]
![[Pasted image 20250117121210.png]]


介绍了基于用户界面生成代码的研究，该研究旨在通过多模态视角，解决现有技术在复杂UI布局理解和代码生成方面的局限性。通过提出一个新的Layout Code模型，重点是提取UI元素、解析布局并生成UI布局树，从而指导生成与原布局一致的HTML文件。该模型在GPT4O上的表现优于之前的工作，并基于实际网站数据构建了新的数据集。
![[Pasted image 20250117121220.png]]
![[Pasted image 20250117121236.png]]
![[Pasted image 20250117121255.png]]
![[Pasted image 20250117121309.png]]
![[Pasted image 20250117121332.png]]
![[Pasted image 20250117121351.png]]


## 三、从基准到实践——大语言模型代码能力初探
> 香港科技大学  曹嘉倫

![[Pasted image 20250117123901.png]]
### 1. 对benchmark的质量分析
研究历时九个月，分析了274篇与代码相关的基准测试（benchmarks），重点关注了这些benchmark的质量问题。研究发现，67%的benchmark没有进行任何质量检查，87.8%的测试案例缺乏对覆盖率的测试，62%的案例未进行错误修复，约10%的benchmark没有开源代码。此外，18%的benchmarks被多次用于其他基准测试的构建，显示出质量问题对后续研究的影响。特别地，human-eval benchmark被多次使用，但存在实现错误，可能对后续研究造成显著影响。

![[Pasted image 20250117124044.png]]
![[Pasted image 20250117124057.png]]
![[Pasted image 20250117124120.png]]

### 2. Benchmark的构建流程及挑战
**流程**：构建、评估、分析结果和发布阶段。  
**挑战**：
- 填补现有的空白
- 覆盖不同编程语言
- 自然语言的能力测试
- 评估方法的选择
- 结果解释的多样性
- 确保复现性的必要信息提供
- 避免发布包含敏感信息的评论

![[Pasted image 20250117124456.png]]

**设计阶段**需要明确所要解决的问题和挑战，以及现有Benchmark在实际测试能力方面存在的不足，如某Python编程能力测试并未充分考察预期的编程技能。
![[Pasted image 20250117124528.png]]


在软件开发的**构建阶段**，质量保证方法显得尤为重要。统计显示，67%的项目未考虑任何质量保证措施，仅22%通过手工检查确保质量，1.5%采用大模型检查，2.2%通过代码执行能力进行验证。对于判断输出正确性的方法，最常用的是考试匹配和通过测试。错误示范包括未被其他论文报告的问题和代码执行问题，即便是通过大模型生成的代码，如果ground truth有误，也无法保证质量。这些发现指出了在软件质量保证实践中存在的问题和挑战。


![[Pasted image 20250117124559.png]]


在大模型**评估阶段**的关键问题。
1. 选择合适数量的大模型进行评估的重要性。统计显示，使用15个大模型能超越全球55%的人，而10个大模型足以覆盖50%。
2. 选择合适的评估方法。发现大多数评估采用零样本（94%），而极少数采用有样本的方法。
3. 实验分析的质量。
	- 大模型评估中的随机性问题和重复实验很重要，却只有3.3%的代码评估会重复实验，因此很多实验结果存在不可信性问题。还
	- 70%的论文提供了良好的分析，但30%缺乏有效分析。
	- 一些研究缺乏有效的复现信息（如prompt）和license信息的不完全提供。
最后，介绍了他们制定的包含55条规则的基准测试指南，覆盖了从构建到发布的全过程，为大模型的评估和发布提供了实践建议。
![[Pasted image 20250117124639.png]]

### 3. 总结
![[Pasted image 20250117125041.png]]