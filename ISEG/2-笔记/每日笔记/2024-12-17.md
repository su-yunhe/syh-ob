# Word2Vec

自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，**词向量是用来表示词的向量**，也可被认为是词的特征向量或表征。**把词映射为实数域向量的技术也叫词嵌入（word embedding）**。

## 1. 为何不采用one-hot向量

- 无法表示语义相似度，任何两个不同词的one-hot向量的余弦相似度都为0；
- 维度太高。

## 2. 跳字模型

### 2.1. 基本概念

跳字模型假设基于某个词来生成它在文本序列周围的词。举个例子，假设文本序列是“the”“man”“loves”“his”“son”。以“loves”作为中心词，设背景窗口大小为2。如图10.1所示，跳字模型所关心的是，给定中心词“loves”，生成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即

P("the","man","his","son"∣"loves").

假设给定中心词的情况下，背景词的生成是相互独立的，那么上式可以改写成

P("the"∣"loves")⋅P("man"∣"loves")⋅P("his"∣"loves")⋅P("son"∣"loves").

![[Pasted image 20241217194707.png]]

在跳字模型中，每个词被表示成两个d维向量，用来计算条件概率。假设这个词在词典中索引为i，当它为中心词时向量表示为$v_i∈ℝ^d$，而为背景词时向量表示为$u_i∈ℝ^d$。设中心词$w_c$在词典中索引为c，背景词$w_o$在词典中索引为o，给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：

$$
 P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}c)}{\sum{i \in V} \exp(\mathbf{u}_i^\top \mathbf{v}_c)}
 $$

假设给定一个长度为T的文本序列，设时间步t的词为$w^{(t)}$。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为m时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率

$$
 \prod_{t=1}^{T} \prod_{-m \le j \le m, j \ne 0} P(w^{(t+j)} \mid w^{(t)})
$$

公式解释

1. 内层求积（背景词概率）：

• 对于每一个中心词  w^{(t)} ，计算它生成前后  m  个词的条件概率。

• 排除  j = 0  的情况，因为  j = 0  时表示的是中心词本身，不计算在内。

• 即，给定中心词  w^{(t)} ，计算它生成所有背景词  w^{(t+j)}  的条件概率。

2. 外层求积（全文本序列）：

• 对于整个文本序列中的每一个时间步  t ，重复上述过程，计算每一个中心词生成其背景词的条件概率。

• 最终，所有的这些概率相乘，得到整个文本序列的联合概率。


示例

  

假设有一个简单的文本序列：“I love learning natural language processing”，窗口大小  m = 2 。

• 对于中心词 “learning”：

• 其前后背景词为 “I”, “love”, “natural”, “language”。

• 计算条件概率  P(I \mid learning) 、 P(love \mid learning) 、 P(natural \mid learning) 、 P(language \mid learning) 。

• 对于整个文本序列中的每一个词（如 “I”, “love”, “learning”, “natural”, “language”, “processing”），重复上述过程，计算所有这些条件概率的乘积，得到整个序列的联合概率。

  

通过这样的训练过程，Word2Vec模型可以生成高质量的词向量，捕捉词汇之间的语义和语法关系。


### 2.2. 模型结构

1. 输入层：一个中心词，表示为一个one-hot向量。

2. 隐藏层：一个隐含层，通常其大小就是你希望得到的词向量的维度。

3. 输出层：与输入层的词汇表大小相同，输出的是每个词作为上下文词的概率。


### 2.3. 模型训练

跳字模型的训练过程包括以下几个步骤：

#### 2.3.1. 准备数据

• 选择一个中心词和它的上下文词对。
• 对于一个给定的文本数据集，生成中心词和上下文词对的训练数据集。

#### 2.3.2. 前向传播

• 输入层是一个one-hot向量，表示中心词。
• 通过隐藏层，将输入词转换为隐含表示，即词向量。
• 通过输出层，计算每个词在上下文中出现的概率。

#### 2.3.3. 损失函数

• 通常使用负采样（Negative Sampling）或层次Softmax（Hierarchical Softmax）来优化计算。
• 损失函数会根据实际的上下文词与预测的概率进行计算，目标是最大化中心词的上下文词概率。

#### 2.3.4. 反向传播

• 通过计算梯度，将误差反向传播到模型的参数中，更新权重。


### 3. 连续词袋模型

连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。在同样的文本序列“the”“man”“loves”“his”“son”里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词“the”“man”“his”“son”生成中心词“loves”的条件概率（如图10.2所示），也就是

P("loves"∣"the","man","his","son").

![[Pasted image 20241217202821.png]]


