

**一、非结构化的模块化方法**

**（一）原来**

         【非结构化方法】以神经元为粒度，基于神经元激活。

         分析和利用神经元在特定任务或特定数据输入下的激活情况，将神经网络划分成多个相对独立的模块。

         问题：

1.     缺乏可解释性+识别不准；

2.     保守删除，仍会保留大量无关权重。

**（二）现在**

【非结构化方法】以权重为粒度。

         粒度更小了。比如卷积层，1个卷积核可以理解为1个神经元，而它包含的**权重数量** **=** **卷积核的大小*********输入通道数****+****偏置项**。显然权重的粒度更小，更精准。

**1.** **模块生成的方法**

**（****1****）** **搜索空间**

         所有掩码的集合。掩码是L维比特向量，第l（0 < l < L）维为0 / 1，表示第 l 个权重被移除/保留了。

         搜索空间被初始化为所有元素值均为1的向量，也就是保留已训练模型所有权重。

**（****2****）** **性能评估策略**

         权重保留率 + 交叉熵损失（加权平均）

         权重保留率越低，说明候选模块保留的权重越少；交叉熵损失越低，说明候选模块在目标数据集上的分类准确度越高，也就说明保留了更多与目标任务相关的权重。

         评估时用原模型 * 掩码得到模块所拥有的所有权重，还要再最后一层加一个全连接层。否则分类个数可能不匹配。         

**（****3****）** **搜索策略**

         基于梯度的离散空间搜索。

         在每一轮搜索中，搜索策略根据前一轮候选模块的目标函数值，通过梯度下降更新掩码和输出头，找到一个具有更小目标函数值的新候选模块。

         输出头是连续的，可以直接进行梯度下降；

         掩码是离散的，所以先将掩码转换成连续的相关度，对相关度进行梯度下降，再用指示函数将相关度换回掩码。

         循环（2）（3）以不断优化，若干轮后输出生成的模块。

 **2.** **模块复用的方法**

**（****1****）** **直接复用**

         模块所具备的功能满足目标任务的需求。

         可使用稀疏模型运行引擎来编译并部署模块，从而提高模块的推理速度。

**（****2****）** **间接复用**

         直接使用目标任务的数据集来精调模块的权重，或使用模块来初始化目标任务模型，再使用目标任务的数据集来训练目标任务模型。

**二、结构化的神经网络模块化方法**

**（一）** **原来**

【非结构化方法】基于神经元激活的方法只适用于全连接神经网络。但是卷积是权重共享的，滑动到输入的每个局部来识别。所以不论识别哪个类别，卷积都一定会被激活，这样就不能用神经元激活来识别模块了。

**（二）** **现在：**

【结构化方法】 以卷积核为粒度

有2种模块生成的方法：基于遗传算法——CNNSplitter；基于梯度下降方法——GradSplitter。主要区别是搜索策略不同。

**1. CNNSplitter**

**（****1****）** **搜索空间**

         理论上需要搜索的是维度等于全部卷积核数量的比特向量集合。但是这样搜索空间太大了。

         论文提出先按照“重要性”将卷积核分组，一位对应一组卷积，这组卷积只能被全部保留/删除，这样搜索空间大幅减小。

         卷积核的“重要性”通过输出的特征图的所有值的总和来衡量。我认为这样的前提是特征图需要先被归一化，丹论文里好像没有明说。

**（****2****）** **搜索策略**

         使用遗传算法。

         初始化的时候先检测这个卷积层的敏感性，从而避免刚初始化完的模块性能太差。具体做法是先随机移除90%卷积核，看看影响，如果精度损失大，说明敏感，这样在初始化时应该少移除一些（10% ～ 50%）；如果精度损失不大，说明不敏感，就多移除一些（50% ～ 90%）。

         之后便按照遗传算法常见的选择、交叉和变异操作迭代地搜索。

**（****3****）** **性能评估策略**

         考虑准确度和差异度。准确度高，说明高内聚；差异度高，说明低耦合。

         准确度就是这个模块（就是一个单类别分类器）的分类准确度。论文中说单类别分类器难以直接计算其对该类别识别的准确度，我还没有太理解。

         差异度指的是模块间两两之间的差异的平均值。计算时把每个模块看作一个卷积核的集合，计算的是集合之间的差异。

**2. GradSplitter**

算法的输入是原始的CNN模型TM和训练数据集，输出是N（即分类的个数）个一一对应的mask和head。

通过mask*TM + head就可以得到针对某个目标类别的模块。

分解过程就是mask和head的训练过程。

训练过程包括：

前向传播——用当前的mask和head构建模块，得到预测结果pred；

反向传播——用梯度下降最小化当前预测 pred 与标签 label 之间的损失，从而来优化mask和head。

**（****1****）** **掩码**

比特向量，指示对应的卷积核保留还是移除。但是梯度下降需要掩码的数值连续，因此训练中使用实数（先初始化成随机的正数表示全部保留），构造模块的时候把大于0的视为1，小于等于0的视为0。

**（****2****）** **输出头**

输出头包括两个全连接网络层和一个 sigmoid 激活函数。从而将N分类预测转换成2分类的。

**（****3****）** **掩码和输出头的优化**

损失函数考虑2方面，loss1（pred 与 label 之间的交叉熵损失，即模块的准确度）和loss2（保留的卷积核占所有卷积核的百分比）。

在最开始的几个轮次只优化输出头，从而恢复由随机初始化输出头引起的准确度损失。

**3.** **模块复用的方法**

**（****1****）** **模型修补**

假如一个CNN模型在一个类别上准确度不高，而另一个模型在该类别上准确度高，就可以用强模型的模块修补弱模型。

修补方法是把原模型和新模块并行地预测，并把对应类别替换。之前还要进行归一化，以消除两个模型可能存在的输出分布差异。

这里，论文中说来自强模型的模块的输出是Op = [op 1, op 2, . . . , op N ]，意思是这个模块没有添加输出头吗？否则应该是一个2分类的，输出0~1之间的单值才对。

**（****2****）** **模型构建**

复用每个已训练模型的最优模块来构建出一个更优的组合模型。

**三、神经网络模型训练时的模块化方法**

在训练的时候便考虑模块化，而不是训练后模块化。这样能保证训练好的模型就已经是高内聚和低耦合的了。

文章定义了神经网络模块化训练中的“内聚度”和“耦合度”。以权重为粒度，将权重分成若干集合。一个集合的权重负责一类样本中的其中一个样本。

内聚度衡量一个模块的权重集合之间的重叠度；耦合度衡量模块之间的权重集合的重叠度。

特别的，针对CNN模型，粒度变更为卷积核，而非单个权重。

训练包含3个关键步骤：识别与输入样本相关的卷积核、评估内聚度与耦合度、优化。

**1.**  **识别相关卷积核**

将一个相关卷积核识别器附加到随机初始化的 CNN 模型上，并与 CNN 模型一起进行训练。具体的实现还没看懂。

**2.** **评估内聚度与耦合度**

单个模块的内聚度和耦合度如前所述。

模型的内聚度是所有模块的平均内聚度；模型的耦合度是所有模块的平均耦合度。

**3.** **优化**

在损失函数中添加模型的内聚度与耦合度。

添加进损失函数时进行余弦相似度转化，目的是将离散的转化成连续的，从而进行梯度下降。